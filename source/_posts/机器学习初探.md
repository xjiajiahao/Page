title: 机器学习初探
date: 2016-08-07 20:50:14
tags: [ml]
categories: Coding
---
　　机器学习目前广泛地运用于电子商务、医疗卫生、机器人、计算机视觉、科学研究等各个领域。机器学习通过对已获得的数据进行学习，可以对未知的信息进行预测、分类或者决策。机器学习在图像识别、语音识别、自然语言理解、个性化推荐、自动统计、决策等应用上表现出色。

　　机器学习的先驱 Arthur Samuel 在1959年把机器学习定义为“让计算机从经验中学习而不需要明确编程"的一个研究领域[^1]。机器学习的任务几乎都是从观察到的数据推断出丢失的或潜在的数据[^2]。目前机器学习最前沿的研究方向有深度学习、概率学习、强化学习等。
<!--more-->
　　监督学习是机器学习最常见的形式。监督学习是指通过对系统外部提供的实例进行推理，作出一般假设，然后对未知的实例进行预测[^3]。首先构造一个目标函数，来测量预测值与真实值之间的误差，然后调节参数使学习模型达到最优。实际工程中经常用到随机梯度下降(SGD)来求解最优的参数，首先从训练集中取一个小样本，计算样本的误差和平均梯度，调整权重，重复这一步骤直到目标函数最优。这种优化方法通常能找到一组不错的参数，而且计算速度快[^4]。

　　而在无监督学习中，机器只获得简单的输入，而没有目标输出值，也不能从环境得到反馈[^5]，这种学习与监督学习相比更接近于人类或动物认识世界的过程。无监督学习能够发现数据的模式和特征，进而可以预测未来的输入，做出决策，可用于聚类和降维。

### 深度学习
　　深度学习的计算模型由多个处理层组成，可以学习多层次抽象的数据的特征[^4]。它与传统的机器学习最大的不同是深度学习可以接受原始的数据，经过通用的学习过程自动发现所需要的数据特征，而不是由工程人员精心设计每一层的特征。由于深度学习不需要太多的工程设计，所以当数据量和计算量增大时，深度学习有明显的优势。目前深度学习在图像识别[^6]、语音识别[^7]和自然语言理解[^8]等很多应用上击败了其他机器学习模型。

　　卷积神经网络(ConvNet)[^9]是一种典型的深度学习模型，它适合处理多矩阵表示的数据，经常用于图像理解，相关的工作有将图片的信息转换为文字[^10]，人脸识别[^11]等。卷积神经网络主要由卷积层和汇集层组成。卷积层的临近的单元组成特征图，特征图中的每个单元连接到上一层的一个局部区域，并共享权值。卷积层的作用是计算局部的特征。汇集层则把语义相似的特征合并。利用反向传播可以对所有层的权重进行训练[^9]。

　　递归神经网络(RNNs)是另外一种深度学习模型，适用于序列形式的输入数据，如文本、语音等。递归神经网络每次处理输入序列的一个元素，并且把元素保存到隐藏单元的状态向量中，以此来记录过去所有元素的历史。

　　LSTM(long short-term memory)网络则是一种效率更高的递归神经网络，LSTM在语音识别和机器翻译等应用上的表现很好。它有一个特别的记忆单元，可以复制自己的状态和积累外部的信号，另外还有一个单元通过学习来决定是否要清除记忆单元的内容。

### 概率学习
　　概率模型为我们理解学习过程提供了一个框架，它向我们展示了如何来表示和控制模型以及预测的不确定性[^2]。模型的不确定性可能来自于测量的误差，参数的选择甚至是模型的选择。用概率的方法建立模型事实上就是用概率论来解释所有形式的不确定性[^12]。随机编程被应用于优化和决策、数据压缩和自动发现数据中的模型、个性化推荐等。

　　贝叶斯学习是用概率论的方法学习的基本模型。它来源于概率论中的贝叶斯公式：
$$ P(y | x) = \frac{P(x | y)P(y)}{\sum_{y \in Y}(x,y)} $$

将贝叶斯公式的 x 和 y 分别替换成机器学习的已知数据 D 和模型 m 中的未知参数 θ，则概率模型可以用下面的公式表示：
$$ P(θ | D, m) = \frac{P(D | θ, m)P(θ | m)}{P(D, m)} $$

其中 $P(θ | m)$ 代表 θ 的先验概率，$P(θ | D, m)$ 代表后验概率[^2]。

　　由于直接对高维度数据的求和以及积分是困难的，因此出现了一些近似积分算法，如马尔科夫链蒙特卡洛(MCMC)，序贯蒙特卡洛等方法。建立模型的难点则在于模型需要足够的灵活。要使模型足够灵活可以采用非参数化的方法。通常可以让一个参数化的模型的参数个数增加到接近无穷大来获得非参数化的模型。典型的模型有高斯过程[^13]和狄利克雷过程等。高斯过程在人脸识别上的表现甚至超过了深度学习[^14]。

　　另一种建模的方法是概率编程[^15]，它的基本思想是用程序表示概率模型。概率编程需要根据观察到的数据逆向计算程序的输入状态，对模型进行不断的调整，这种调整只需要用通用的推理机，就能优雅地实现。概率编程在机器学习和科学建模上都是革命性的[^2]，它的思想与传统的机器学习和科学范式有本质的不同。

### 强化学习
　　强化学习关注的是通过与环境进行互动获得经验和反馈，从而提高系统做出行为决策的能力[^16]。强化学习的程序只需要有奖励和惩罚的机制，而不需要明确地指出应该如何完成一项任务[^17]。它的灵感来源是人类和动物做出决策得到奖励的学习过程。

　　强化学习的方法有时间差分学习、规划（planning，如马尔科夫决策、评价函数、蒙特卡洛搜索树）和基于模型的强化学习等。

　　时间差分学习通过计算临时的连续预测结果的差值[^18]，可以降低计算量，产生更准确的预测。评价函数用于博弈树中，程序先尽可能搜索到博弈树的深处，然后用评价函数来评估博弈如何结束。而蒙特卡洛搜索树(MCTS)则总是对下一个行动是否采取某个策略进行评估，找出一个合适的子集，减小搜索的宽度。

　　基于模型的强化学习利用经验来构建迁移模型，然后用规划的方法进行决策。这种学习模型计算量大，但充分利用了每一次经验[^16]，适用于经验不容易获得的场景。

### 未来展望
　　机器学习将在人工智能领域取得更大的成功，尤其是在自然语言理解、科学研究模型。而且随着数据量的迅速膨胀，特征学习、复杂推理和概率学习将变得越来越重要，这会使人类工程设计的工作大大减轻，机器学习对人类手工设计模型的依赖程度会越来越低。未来将出现更加泛化的学习模型和新的范式，以解决更为一般化的问题。

[^4]: LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. *Nature* **521**, 436–444 (2015).

[^2]: Ghahramani, Z. Probabilistic machine learning and artificial intelligence. *Nature* **521**, 452–459 (2015).

[^16]: Littman, M. L. Reinforcement learning improves behaviour from evaluative feedback. *Nature* **521**, 445-451 (2015).

 [^6]: Szegedy, C. et al. Going deeper with convolutions. Preprint at http://arxiv.org/abs/1409.4842 (2014).

[^7]: Hinton, G. et al. Deep neural networks for acoustic modeling in speech recognition. *IEEE Signal Processing Magazine* **29**, 82–97 (2012).

[^8]: Collobert, R., *et al.* Natural language processing (almost) from scratch. *J. Mach. Learn. Res.* **12**, 2493–2537 (2011).

[^9]: LeCun, Y., Bottou, L., Bengio, Y. & Haffner, P. Gradient-based learning applied to document recognition. *Proc. IEEE* **86**, 2278–2324 (1998).

[^10]: Xu, K., *et al.* Show, Attend and Tell: Neural image caption generation with visual attention. Preprint at http://arxiv.org/abs/1502.03044 (2015).

[^11]: Taigman, Y., Yang, M., Ranzato, M. & Wolf, L. Deepface: closing the gap to human-level performance in face veriﬁcation. *In Proc. Conference on Computer Vision and Pattern Recognition* 1701–1708 (2014).

[^12]: Ghahramani, Z. Bayesian nonparametrics and the probabilistic approach to modelling. *Phil. Trans. R. Soc. A* **371**, 20110553 (2013).

[^13]: Rasmussen, C. E. & Williams, C. K. I. *Gaussian Processes for Machine Learning* (MIT Press, 2006).

[^15]: Pfeffer, A. *Practical Probabilistic Programming* (Manning, 2015).

[^14]: Lu, C. & Tang, X. Surpassing human-level face verification performance on LFW with GaussianFace. In *Proc. 29th AAAI Conference on Artificial Intelligence *http://arxiv.org/abs/1404.3840 (2015).

[^17]: Kaelbling, L. P., Littman, M. L. & Moore, A. W. Reinforcement learning: a survey. *J. Artif. Intell. Res.* **4**, 237–285 (1996).

[^18]: Sutton, R. S. Learning to predict by the method of temporal differences. *Mach. Learn.* **3**, 9–44 (1988).

[^1]: Samuel, A. L. Some studies in machine learning using the game of checkers. Reprint at *IBM J. RES. DEVELOP.* **44**, 207-226 (2000).

[^3]: Kotsiantis, S. B. Supervised machine learning: a review of classification techniques *Informatica* **31** (2007) 249-268

[^5]: Ghahramani, Z. Unsupervised learning *Adv. Lect. Mach. Learn.* 72-112 (2004).
